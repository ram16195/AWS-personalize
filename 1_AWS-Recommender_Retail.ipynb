{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Recommender for B2B-Retail with AWS Personalize\n",
    "\n",
    "Building a recommender system with AWS Personalize using the SDK for Python option (boto3). The data is the same longtail B2B-Retail set as in the \"Association Rules Mining\" ML-Project, but this time I don't reduce it to the approx 3'000 most popular items. I upload the full set.\n",
    "\n",
    "[Documentation](https://docs.aws.amazon.com/personalize/latest/dg/what-is-personalize.html) for AWS Personalize.\n",
    "\n",
    "I learned the hard way:\n",
    "- For Europe AWS Personalize is only available in Region Ireland (eu-west-1), this is important when configuring the AWSCLI.\n",
    "- Timestamp col in interactions dataset has to be in int format\n",
    "\n",
    "\n",
    "**Data Sources:**\n",
    "\n",
    "- `data/raw/sales_total.csv`: Transaction data ('sales log') for 2017/18, this is the main data file representing the interactions between users and items.\n",
    "- `data/raw/customers_agg_2018.csv`: (Optional) data containing metadata for the users (their respective business sector).\n",
    "\n",
    "\n",
    "**Data Output:**\n",
    "\n",
    "- `xxx.csv`: blablabla\n",
    "\n",
    "**Changes**\n",
    "\n",
    "- 2019-07-18: Start project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries,-load-data\" data-toc-modified-id=\"Import-libraries,-load-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import libraries, load data</a></span></li><li><span><a href=\"#Prepare-and-upload-training-data-to-S3-bucket\" data-toc-modified-id=\"Prepare-and-upload-training-data-to-S3-bucket-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Prepare and upload training data to S3 bucket</a></span><ul class=\"toc-item\"><li><span><a href=\"#Upload-data-to-S3-bucket\" data-toc-modified-id=\"Upload-data-to-S3-bucket-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Upload data to S3 bucket</a></span></li></ul></li><li><span><a href=\"#Prepare-Data-Structure\" data-toc-modified-id=\"Prepare-Data-Structure-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Prepare Data Structure</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-Schemas\" data-toc-modified-id=\"Create-Schemas-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Create Schemas</a></span></li><li><span><a href=\"#Create-(and-wait-for)-Dataset-Group\" data-toc-modified-id=\"Create-(and-wait-for)-Dataset-Group-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Create (and wait for) Dataset Group</a></span></li><li><span><a href=\"#Create-Datasets\" data-toc-modified-id=\"Create-Datasets-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Create Datasets</a></span></li></ul></li><li><span><a href=\"#Prepare,-create,-and-wait-for-Dataset-Import-Job\" data-toc-modified-id=\"Prepare,-create,-and-wait-for-Dataset-Import-Job-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Prepare, create, and wait for Dataset Import Job</a></span></li><li><span><a href=\"#Select-a-Recipe-(for-demo-only)\" data-toc-modified-id=\"Select-a-Recipe-(for-demo-only)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Select a Recipe (for demo only)</a></span></li><li><span><a href=\"#Create-and-Wait-for-Solution-(version)\" data-toc-modified-id=\"Create-and-Wait-for-Solution-(version)-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Create and Wait for Solution (version)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries, load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T20:14:10.575455Z",
     "start_time": "2019-07-20T20:14:03.612013Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries, get personalize boto3 client\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')\n",
    "\n",
    "# Display settings\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group deleted\n",
      "schema deleted\n"
     ]
    }
   ],
   "source": [
    "# First things first: Delete all existing resources before (re-)running project\n",
    "\n",
    "group = 'recommender-test-dataset-group'\n",
    "set_list = ['INTERACTIONS', 'USERS']\n",
    "schema_list = ['interactions-schema', 'users-schema']\n",
    "\n",
    "try:\n",
    "    for set in set_list:\n",
    "        personalize.delete_dataset(\n",
    "            datasetArn=\"arn:aws:personalize:eu-west-1:873674308518:dataset/{}/{}\".format(group, set))\n",
    "        print('set deleted')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    personalize.delete_dataset_group(\n",
    "        datasetGroupArn=\"arn:aws:personalize:eu-west-1:873674308518:dataset-group/{}\".format(group))\n",
    "    print('group deleted')\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "    \n",
    "try:\n",
    "    for schema in ['interactions-schema', 'users-schema']:\n",
    "        personalize.delete_schema(\n",
    "            schemaArn=\"arn:aws:personalize:eu-west-1:873674308518:schema/{}\".format(schema))\n",
    "        print('schema deleted')\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "interactions_raw = pd.read_csv('data/raw/sales_total.csv', parse_dates=['Fakturadatum'])\n",
    "users_raw = pd.read_csv('data/raw/customers_agg_2018.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T14:25:07.736597Z",
     "start_time": "2019-07-18T14:25:05.697778Z"
    }
   },
   "source": [
    "## Prepare and upload training data to S3 bucket\n",
    "\n",
    "Check documentation for more info. As we have no relevant metadata for items, we prepare 2 datasets for\n",
    "\n",
    "- interactions\n",
    "- users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\r2d4\\Anaconda3\\envs\\aws\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\r2d4\\Anaconda3\\envs\\aws\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "C:\\Users\\r2d4\\Anaconda3\\envs\\aws\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\r2d4\\Anaconda3\\envs\\aws\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\r2d4\\Anaconda3\\envs\\aws\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\r2d4\\Anaconda3\\envs\\aws\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Prepare interaction data\"\"\"\n",
    "\n",
    "# Subset data for 2018 data only\n",
    "interactions_18_full = interactions_raw.loc[interactions_raw['Fakturadatum'].dt.year == 2018]\n",
    "interactions_18_part = interactions_18_full[['Kunde', 'Artikel', 'Fakturadatum', 'Nettowert']]\n",
    "\n",
    "# Kick out all artikel that contain str values in their code\n",
    "interactions_18_part['num'] = pd.to_numeric(interactions_18_part['Artikel'], errors='coerce')\n",
    "interactions_18 = interactions_18_part.dropna(how='any')\n",
    "interactions_18.drop(['num'], axis=1, inplace=True)\n",
    "\n",
    "# Kick-out special customers\n",
    "interactions = interactions_18.loc[interactions_18['Kunde'] > 700000]\n",
    "\n",
    "# Set datatypes\n",
    "interactions['Kunde'] = interactions['Kunde'].astype(str)\n",
    "interactions['Artikel'] = interactions['Artikel'].astype(str)\n",
    "interactions['Fakturadatum'] = interactions['Fakturadatum'].apply(lambda x: x.timestamp()).astype(int)\n",
    "interactions['Nettowert'] = interactions['Nettowert'].astype(float)\n",
    "\n",
    "# Rename Columns\n",
    "interactions = interactions.rename(columns={'Kunde': 'USER_ID', \n",
    "                                            'Artikel': 'ITEM_ID',\n",
    "                                            'Fakturadatum': 'TIMESTAMP',\n",
    "                                            'Nettowert': 'EVENT_VALUE',\n",
    "                                           })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1402641, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>EVENT_VALUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1388625</th>\n",
       "      <td>8488019</td>\n",
       "      <td>5171607</td>\n",
       "      <td>1514937600</td>\n",
       "      <td>77.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388626</th>\n",
       "      <td>8488019</td>\n",
       "      <td>5171101</td>\n",
       "      <td>1514937600</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         USER_ID  ITEM_ID   TIMESTAMP  EVENT_VALUE\n",
       "1388625  8488019  5171607  1514937600         77.3\n",
       "1388626  8488019  5171101  1514937600         32.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check results\n",
    "assert interactions.isnull().sum().sum() == 0\n",
    "print(interactions.shape)\n",
    "display(interactions.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "interactions.to_csv(\"data/interim/interactions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18625, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>BRANCHE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8107232</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8155006</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   USER_ID BRANCHE\n",
       "0  8107232    15.0\n",
       "1  8155006    10.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Prepare User data\"\"\"\n",
    "\n",
    "users = users_raw[['Unnamed: 0', 'Branche']]\n",
    "users = users.rename(columns={'Unnamed: 0': 'USER_ID', \n",
    "                              'Branche': 'BRANCHE',\n",
    "                             })\n",
    "\n",
    "# Set datatypes\n",
    "users['USER_ID'] = users['USER_ID'].astype(str)\n",
    "users['BRANCHE'] = users['BRANCHE'].astype(str)\n",
    "\n",
    "# Check results\n",
    "print(users.shape)\n",
    "display(users.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "users.to_csv(\"data/interim/users.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbuerki-01-personalize\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the list of existing buckets (optional)\n",
    "s3 = boto3.client('s3')\n",
    "response= s3.list_buckets()\n",
    "for bucket in response['Buckets']:\n",
    "    print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '3350DA265FA1CEB4',\n",
       "  'HostId': 'JBwGOMbCtCpluYN0M0/K3cAxEcIjH4nyVsZmJVsEoYX5bkFjRpFKVLrCncGBesMXlW6PQHk5ah4=',\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'JBwGOMbCtCpluYN0M0/K3cAxEcIjH4nyVsZmJVsEoYX5bkFjRpFKVLrCncGBesMXlW6PQHk5ah4=',\n",
       "   'x-amz-request-id': '3350DA265FA1CEB4',\n",
       "   'date': 'Tue, 23 Jul 2019 03:23:29 GMT',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 1}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Specify a s3 Bucket and attach policy to it\"\"\"\n",
    "\n",
    "bucket = \"rbuerki-01-personalize\"  # name of my S3 bucket\n",
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket, Policy=json.dumps(policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T20:12:10.749057Z",
     "start_time": "2019-07-20T20:12:10.733435Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Upload interactions data\"\"\"\n",
    "\n",
    "filename_i = 'interactions.csv' \n",
    "# boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "#     filename_i).upload_file(\"data/interim/{}\".format(filename_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Upload user data\"\"\"\n",
    "\n",
    "filename_u = 'users.csv'\n",
    "# boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "#     filename_u).upload_file(\"data/interim/{}\".format(filename_u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data Structure\n",
    "\n",
    "### Create Schemas\n",
    "\n",
    "Schemas in Amazon Personalize are defined in the Avro format. For more information, see [Apache Avro](https://avro.apache.org/docs/current/). The schema fields can be in any order but must match the order of the corresponding column headers in the data files to be imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_schema = {\"type\": \"record\", \n",
    "                       \"name\": \"Interactions\",\n",
    "                       \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "                       \"fields\": [\n",
    "                       {\n",
    "                           \"name\": \"USER_ID\",\n",
    "                           \"type\": \"string\"\n",
    "                       },\n",
    "                       {\n",
    "                           \"name\": \"ITEM_ID\",\n",
    "                           \"type\": \"string\"\n",
    "                       },\n",
    "                       {\n",
    "                           \"name\": \"TIMESTAMP\",\n",
    "                           \"type\": \"long\"\n",
    "                       },\n",
    "                       {\n",
    "                           \"name\": \"EVENT_VALUE\",\n",
    "                           \"type\": \"float\"\n",
    "                       }\n",
    "                                  ],\n",
    "                                  \"version\": \"1.0\"\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schemaArn\": \"arn:aws:personalize:eu-west-1:873674308518:schema/interactions-schema\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"1f74fbc7-3096-447a-aa12-085bbbf446ad\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Tue, 23 Jul 2019 03:23:34 GMT\",\n",
      "      \"x-amzn-requestid\": \"1f74fbc7-3096-447a-aa12-085bbbf446ad\",\n",
      "      \"content-length\": \"85\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"interactions-schema\",\n",
    "    schema = json.dumps(interactions_schema)\n",
    ")\n",
    "\n",
    "interactions_schema_arn = create_schema_response['schemaArn']\n",
    "print(json.dumps(create_schema_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_schema = {\"type\": \"record\", \n",
    "                \"name\": \"Users\",\n",
    "                \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "                \"fields\": [\n",
    "                {\n",
    "                    \"name\": \"USER_ID\",\n",
    "                    \"type\": \"string\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"BRANCHE\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"categorical\": True\n",
    "                }\n",
    "                          ],\n",
    "                          \"version\": \"1.0\"\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schemaArn\": \"arn:aws:personalize:eu-west-1:873674308518:schema/users-schema\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"1f9c457f-6a7e-4716-ac6e-b69002ff78c0\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Tue, 23 Jul 2019 03:23:35 GMT\",\n",
      "      \"x-amzn-requestid\": \"1f9c457f-6a7e-4716-ac6e-b69002ff78c0\",\n",
      "      \"content-length\": \"78\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"users-schema\",\n",
    "    schema = json.dumps(users_schema)\n",
    ")\n",
    "\n",
    "users_schema_arn = create_schema_response['schemaArn']\n",
    "print(json.dumps(create_schema_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create (and wait for) Dataset Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetGroupArn\": \"arn:aws:personalize:eu-west-1:873674308518:dataset-group/recommender-test-dataset-group\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"df62824c-dec3-430b-8553-0385642a91e9\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Tue, 23 Jul 2019 03:23:37 GMT\",\n",
      "      \"x-amzn-requestid\": \"df62824c-dec3-430b-8553-0385642a91e9\",\n",
      "      \"content-length\": \"109\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_dataset_group_response = personalize.create_dataset_group(\n",
    "    name = \"recommender-test-dataset-group\"\n",
    ")\n",
    "\n",
    "dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "print(json.dumps(create_dataset_group_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetGroup: CREATE PENDING\n",
      "DatasetGroup: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Wait for Dataset Group to have ACTIVE status\"\"\"\n",
    "\n",
    "max_time = time.time() + 3*60 # 3 minutes\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:eu-west-1:873674308518:dataset/recommender-test-dataset-group/INTERACTIONS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"e06c5ca8-08c8-412e-a02d-14880c856a05\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Tue, 23 Jul 2019 03:24:14 GMT\",\n",
      "      \"x-amzn-requestid\": \"e06c5ca8-08c8-412e-a02d-14880c856a05\",\n",
      "      \"content-length\": \"111\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset_type = \"INTERACTIONS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    name = \"recommender-test-interactions\",\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = interactions_schema_arn\n",
    ")\n",
    "\n",
    "dataset_arn_i = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:eu-west-1:873674308518:dataset/recommender-test-dataset-group/USERS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"12ec0533-eacf-491c-8749-3fb265df205d\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Tue, 23 Jul 2019 03:24:16 GMT\",\n",
      "      \"x-amzn-requestid\": \"12ec0533-eacf-491c-8749-3fb265df205d\",\n",
      "      \"content-length\": \"104\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset_type = \"USERS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    name = \"recommender-test-users\",\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = users_schema_arn\n",
    ")\n",
    "\n",
    "dataset_arn_u = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare, create, and wait for Dataset Import Job\n",
    "\n",
    "Because I have initially already set up a Personalize role (see documentation) the first code cell is inactive and I simply load the roleArn in the second code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Create Personalize role\"\"\"\n",
    "\n",
    "# iam = boto3.client(\"iam\")\n",
    "\n",
    "# role_name = \"PersonalizeRole\"\n",
    "# assume_role_policy_document = {\n",
    "#     \"Version\": \"2012-10-17\",\n",
    "#     \"Statement\": [\n",
    "#         {\n",
    "#           \"Effect\": \"Allow\",\n",
    "#           \"Principal\": {\n",
    "#             \"Service\": \"personalize.amazonaws.com\"\n",
    "#           },\n",
    "#           \"Action\": \"sts:AssumeRole\"\n",
    "#         }\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# create_role_response = iam.create_role(\n",
    "#     RoleName = role_name,\n",
    "#     AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    "# )\n",
    "\n",
    "# # AmazonPersonalizeFullAccess provides access to any S3 bucket with a name that includes \"personalize\" or \"Personalize\" \n",
    "# # if you would like to use a bucket with a different name, please consider creating and attaching a new policy\n",
    "# # that provides read access to your bucket or attaching the AmazonS3ReadOnlyAccess policy to the role\n",
    "# policy_arn = \"arn:aws:iam::aws:policy/service-role/AmazonPersonalizeFullAccess\"\n",
    "# iam.attach_role_policy(\n",
    "#     RoleName = role_name,\n",
    "#     PolicyArn = policy_arn\n",
    "# )\n",
    "\n",
    "# time.sleep(60) # wait for a minute to allow IAM role policy attachment to propagate\n",
    "\n",
    "# role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "# print(role_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_arn = \"arn:aws:iam::873674308518:role/PersonalizeRole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:eu-west-1:873674308518:dataset-import-job/interactions-dataset-import-job\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"358063ac-614b-4985-a10e-8f9e99653867\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Tue, 23 Jul 2019 03:24:22 GMT\",\n",
      "      \"x-amzn-requestid\": \"358063ac-614b-4985-a10e-8f9e99653867\",\n",
      "      \"content-length\": \"119\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create dataset import job for interactions\"\"\"\n",
    "\n",
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"interactions-dataset-import-job\",\n",
    "    datasetArn = dataset_arn_i,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket, filename_i)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "dataset_import_job_arn_i = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Wait for Dataset Import Job to Have ACTIVE Status\"\"\"\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn_i\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"DatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:eu-west-1:873674308518:dataset-import-job/users-dataset-import-job\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"f8ad25c3-c03c-472f-baae-219227e9490f\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Tue, 23 Jul 2019 03:28:42 GMT\",\n",
      "      \"x-amzn-requestid\": \"f8ad25c3-c03c-472f-baae-219227e9490f\",\n",
      "      \"content-length\": \"112\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create dataset import job for users\"\"\"\n",
    "\n",
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"users-dataset-import-job\",\n",
    "    datasetArn = dataset_arn_u,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket, filename_u)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "dataset_import_job_arn_u = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetImportJob: CREATE PENDING\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Wait for Dataset Import Job to Have ACTIVE Status\"\"\"\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn_u\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"DatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a Recipe (for demo only)\n",
    "\n",
    "A _recipe_ in Amazon Personalize is made up of an algorithm with hyperparameters, and a feature transformation. Amazon Personalize provides a number of predefined recipes that allow you to make recommendations with no knowledge of machine learning.The predefined recipes are also useful for quick experimentation.\n",
    "\n",
    "(To customize the training, supply the `solutionConfig` parameter. The SolutionConfig object allows you to override the default solution and recipe parameters. This is not done here.)\n",
    "\n",
    "**NOTE:** _For this case I won't use a predefined recipe, I will let Personalize choose the optimal algorithm in the next step by calling `createSolution` with param `autoML=True`. Therefore the next codeblock is inactivated. See demo notebook for use of a predefined recipe._\n",
    "\n",
    "[docs](https://docs.aws.amazon.com/personalize/latest/dg/training-deploying-solutions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T16:29:03.047162Z",
     "start_time": "2019-07-23T16:29:03.032888Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"For demo purpose only: select an aws-hrnn\"\"\"\n",
    "\n",
    "# list_recipes_response = personalize.list_recipes()\n",
    "# recipe_arn = \"arn:aws:personalize:::recipe/aws-hrnn\"\n",
    "# list_recipes_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Wait for Solution (version)\n",
    "\n",
    "Creating a solution entails optimizing the model to deliver the best results for a specific business need. Amazon Personalize uses \"recipes\" to create these personalized solutions. (Altough in this specific case no pre-definied recipe is used.) A _solution version_ is the term Amazon Personalize uses for a trained machine learning model that makes recommendations to customers. \n",
    "\n",
    "A solution is created by calling the `CreateSolution` and `CreateSolutionVersion` operations. CreateSolution creates the configuration for training a model. CreateSolutionVersion starts the training process, which results in a specific version of the solution.\n",
    "\n",
    "[docs](https://docs.aws.amazon.com/personalize/latest/dg/training-deploying-solutions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create solution\"\"\"\n",
    "\n",
    "print ('Creating solution')\n",
    "response = personalize.create_solution(\n",
    "    name = \"recommender-test-solution\",\n",
    "    datasetGroupArn = \"recommender-test-dataset-group\",\n",
    "    performAutoML = True)\n",
    "\n",
    "# Get the solution ARN.\n",
    "solution_arn = response['solutionArn']\n",
    "print('Solution ARN: ' + solution_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wait for solution to have ACTIVE status\"\"\"\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    # Use the solution ARN to get the solution status.\n",
    "    solution_description = personalize.describe_solution(solutionArn = solution_arn)['solution']\n",
    "    print('Solution status: ' + solution_description['status'])\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create solution version\"\"\"\n",
    "\n",
    "# Use the solution ARN to create a solution version.\n",
    "print ('Creating solution version')\n",
    "response = personalize.create_solution_version(solutionArn = solution_arn)\n",
    "solution_version_arn = response['solutionVersionArn']\n",
    "print('Solution version ARN: ' + solution_version_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save / load solution Version ARN for convenience\n",
    "\n",
    "%store solution_version_arn\n",
    "# %store -r solution_version_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Check solution version status (manually)\"\"\"\n",
    "\n",
    "# Use the solution version ARN to get the solution version status.\n",
    "solution_version_description = personalize.describe_solution_version(\n",
    "    solutionVersionArn = solution_version_arn)['solutionVersion']\n",
    "print('Solution version status: ' + solution_version_description['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- please note in initial comments that we use hrnn algorithm\n",
    "- generally update the comments with an overview of what we are exactly doing (boto vs. console vs. prompt)\n",
    "- for me: check what else would be possible to build a more complex model in the end"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
