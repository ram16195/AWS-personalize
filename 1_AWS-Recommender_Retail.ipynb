{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Recommender for B2B-Retail with AWS Personalize\n",
    "\n",
    "Building a recommender system with AWS Personalize using the SDK for Python option (boto3). The data is the same longtail B2B-Retail set as in the \"Association Rules Mining\" ML-Project, but this time I don't reduce it to the approx 3'000 most popular items. I upload the full set.\n",
    "\n",
    "[Documentation](https://docs.aws.amazon.com/personalize/latest/dg/what-is-personalize.html) for AWS Personalize.\n",
    "\n",
    "I learned the hard way:\n",
    "- For Europe AWS Personalize is only available in Region Ireland (eu-west-1), this is important when configuring the AWSCLI.\n",
    "- Timestamp col in interactions dataset has to be in int format\n",
    "\n",
    "\n",
    "**Data Sources:**\n",
    "\n",
    "- `data/raw/sales_total.csv`: Transaction data ('sales log') for 2017/18, this is the main data file representing the interactions between users and items.\n",
    "- `data/raw/customers_agg_2018.csv`: (Optional) data containing metadata for the users (meaning their respective business sector).\n",
    "- `data/raw/artikel_agg_2018.csv`: (Optional) data containing the names of the artikel, only needed for final output.\n",
    "\n",
    "**Changes**\n",
    "\n",
    "- 2019-07-18: Start project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries,-load-data\" data-toc-modified-id=\"Import-libraries,-load-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import libraries, load data</a></span></li><li><span><a href=\"#Prepare-and-upload-training-data-to-S3-bucket\" data-toc-modified-id=\"Prepare-and-upload-training-data-to-S3-bucket-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Prepare and upload training data to S3 bucket</a></span><ul class=\"toc-item\"><li><span><a href=\"#Upload-data-to-S3-bucket\" data-toc-modified-id=\"Upload-data-to-S3-bucket-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Upload data to S3 bucket</a></span></li></ul></li><li><span><a href=\"#Prepare-Data-Structure\" data-toc-modified-id=\"Prepare-Data-Structure-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Prepare Data Structure</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-Schemas\" data-toc-modified-id=\"Create-Schemas-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Create Schemas</a></span></li><li><span><a href=\"#Create-(and-wait-for)-Dataset-Group\" data-toc-modified-id=\"Create-(and-wait-for)-Dataset-Group-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Create (and wait for) Dataset Group</a></span></li><li><span><a href=\"#Create-Datasets\" data-toc-modified-id=\"Create-Datasets-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Create Datasets</a></span></li></ul></li><li><span><a href=\"#Prepare,-create,-and-wait-for-Dataset-Import-Job\" data-toc-modified-id=\"Prepare,-create,-and-wait-for-Dataset-Import-Job-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Prepare, create, and wait for Dataset Import Job</a></span></li><li><span><a href=\"#Select-a-Recipe-(for-demo-only)\" data-toc-modified-id=\"Select-a-Recipe-(for-demo-only)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Select a Recipe (for demo only)</a></span></li><li><span><a href=\"#Create-and-Wait-for-Solution-(version)\" data-toc-modified-id=\"Create-and-Wait-for-Solution-(version)-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Create and Wait for Solution (version)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries, load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T20:14:10.575455Z",
     "start_time": "2019-07-20T20:14:03.612013Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries, get personalize boto3 client\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')\n",
    "\n",
    "# Display settings\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set deleted\n",
      "set deleted\n"
     ]
    }
   ],
   "source": [
    "# First things first: Delete existing resources before (re-)running the project\n",
    "group = 'recommender-test-dataset-group'\n",
    "set_list = ['INTERACTIONS', 'USERS']\n",
    "schema_list = ['interactions-schema', 'users-schema']\n",
    "\n",
    "try:\n",
    "    for set in set_list:\n",
    "        personalize.delete_dataset(\n",
    "            datasetArn=\"arn:aws:personalize:eu-west-1:873674308518:dataset/{}/{}\".format(group, set))\n",
    "        print('set deleted')\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    personalize.delete_dataset_group(\n",
    "        datasetGroupArn=\"arn:aws:personalize:eu-west-1:873674308518:dataset-group/{}\".format(group))\n",
    "    print('group deleted')\n",
    "except Exception:\n",
    "    pass  \n",
    "try:\n",
    "    for schema in ['interactions-schema', 'users-schema']:\n",
    "        personalize.delete_schema(\n",
    "            schemaArn=\"arn:aws:personalize:eu-west-1:873674308518:schema/{}\".format(schema))\n",
    "        print('schema deleted')\n",
    "except Exception:\n",
    "    pass\n",
    "    \n",
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "interactions_raw = pd.read_csv('data/raw/sales_total.csv', parse_dates=['Fakturadatum'])\n",
    "users_raw = pd.read_csv('data/raw/customers_agg_2018.csv')\n",
    "artikel_raw = pd.read_csv('data/raw/artikel_agg_2018.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T14:25:07.736597Z",
     "start_time": "2019-07-18T14:25:05.697778Z"
    }
   },
   "source": [
    "## Prepare and upload training data to S3 bucket\n",
    "\n",
    "Amazon Personalize recognizes three types of historical datasets. Each type has an associated schema (see next section) with a name key whose value matches the dataset type. The three types are: \n",
    "- **Users:** This dataset is intended to provide metadata about your users. This includes information such as age, gender, and loyalty membership, among others, which can be important signals in personalization systems. \n",
    "- **Items:** This dataset is intended to provide metadata about your items. This includes information such as price, SKU type, and availability, among others. \n",
    "- **Interactions:** This dataset is intended to provide historical interaction data between users and items. \n",
    "\n",
    "The Users and Items dataset types are known as metadata types and are only used by certain recipes. As we have no relevant metadata for items, we prepare 2 datasets for\n",
    "\n",
    "- interactions\n",
    "- users\n",
    "\n",
    "[docs](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prepare interaction data\"\"\"\n",
    "\n",
    "# Subset data for 2018 data only\n",
    "interactions_18_full = interactions_raw.loc[interactions_raw['Fakturadatum'].dt.year == 2018]\n",
    "interactions_18_part = interactions_18_full[['Kunde', 'Artikel', 'Fakturadatum', 'Nettowert']]\n",
    "\n",
    "# Kick out all artikel that contain str values in their code\n",
    "interactions_18_part['num'] = pd.to_numeric(interactions_18_part['Artikel'], errors='coerce')\n",
    "interactions_18 = interactions_18_part.dropna(how='any')\n",
    "interactions_18.drop(['num'], axis=1, inplace=True)\n",
    "\n",
    "# Kick-out special customers\n",
    "interactions = interactions_18.loc[interactions_18['Kunde'] > 700000]\n",
    "\n",
    "# Set datatypes\n",
    "interactions['Kunde'] = interactions['Kunde'].astype(str)\n",
    "interactions['Artikel'] = interactions['Artikel'].astype(str)\n",
    "interactions['Fakturadatum'] = interactions['Fakturadatum'].apply(lambda x: x.timestamp()).astype(int)\n",
    "interactions['Nettowert'] = interactions['Nettowert'].astype(float)\n",
    "\n",
    "# Rename Columns\n",
    "interactions = interactions.rename(columns={'Kunde': 'USER_ID', \n",
    "                                            'Artikel': 'ITEM_ID',\n",
    "                                            'Fakturadatum': 'TIMESTAMP',\n",
    "                                            'Nettowert': 'EVENT_VALUE',\n",
    "                                           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1402641, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>EVENT_VALUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1388625</th>\n",
       "      <td>8488019</td>\n",
       "      <td>5171607</td>\n",
       "      <td>1514937600</td>\n",
       "      <td>77.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388626</th>\n",
       "      <td>8488019</td>\n",
       "      <td>5171101</td>\n",
       "      <td>1514937600</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         USER_ID  ITEM_ID   TIMESTAMP  EVENT_VALUE\n",
       "1388625  8488019  5171607  1514937600         77.3\n",
       "1388626  8488019  5171101  1514937600         32.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check results\n",
    "assert interactions.isnull().sum().sum() == 0\n",
    "print(interactions.shape)\n",
    "display(interactions.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "interactions.to_csv(\"data/interim/interactions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18625, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>BRANCHE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8107232</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8155006</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   USER_ID BRANCHE\n",
       "0  8107232    15.0\n",
       "1  8155006    10.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Prepare User data\"\"\"\n",
    "\n",
    "users = users_raw[['Unnamed: 0', 'Branche']]\n",
    "users = users.rename(columns={'Unnamed: 0': 'USER_ID', \n",
    "                              'Branche': 'BRANCHE',\n",
    "                             })\n",
    "\n",
    "# Set datatypes\n",
    "users['USER_ID'] = users['USER_ID'].astype(str)\n",
    "users['BRANCHE'] = users['BRANCHE'].astype(str)\n",
    "\n",
    "# Check results\n",
    "print(users.shape)\n",
    "display(users.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "users.to_csv(\"data/interim/users.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3 bucket\n",
    "\n",
    "After you create a CSV file with your data, upload the file to your Amazon S3 bucket. This is the location that Amazon Personalize imports your data from. Amazon Personalize needs permission to access the Amazon S3 bucket, so a policy has to be attached.\n",
    "\n",
    "[docs](https://docs.aws.amazon.com/personalize/latest/dg/data-prep-upload-s3.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbuerki-01-personalize\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the list of existing buckets (optional)\n",
    "s3 = boto3.client('s3')\n",
    "response= s3.list_buckets()\n",
    "for bucket in response['Buckets']:\n",
    "    print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '030FB1FFE142AC6A',\n",
       "  'HostId': '4XuQgttWMNkZZD+L+01WTuAffnkcRqel2xPYbCar51OoPj/qokeEEiUQ1hkfYVM7lyGJ419gKbw=',\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '4XuQgttWMNkZZD+L+01WTuAffnkcRqel2xPYbCar51OoPj/qokeEEiUQ1hkfYVM7lyGJ419gKbw=',\n",
       "   'x-amz-request-id': '030FB1FFE142AC6A',\n",
       "   'date': 'Tue, 23 Jul 2019 19:54:39 GMT',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 1}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Specify a s3 Bucket and attach policy to it\"\"\"\n",
    "\n",
    "bucket = \"rbuerki-01-personalize\"  # name of my S3 bucket\n",
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket, Policy=json.dumps(policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T20:12:10.749057Z",
     "start_time": "2019-07-20T20:12:10.733435Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Upload interactions data\"\"\"\n",
    "\n",
    "filename_i = 'interactions.csv' \n",
    "# boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "#     filename_i).upload_file(\"data/interim/{}\".format(filename_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Upload user data\"\"\"\n",
    "\n",
    "filename_u = 'users.csv'\n",
    "# boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "#     filename_u).upload_file(\"data/interim/{}\".format(filename_u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data Structure\n",
    "\n",
    "\n",
    "Import your training data into Amazon Personalize by first creating matching data schemas for your sets, then an empty dataset group and then an empty dataset in that dataset group. Next, create an import job that populates the dataset with data from your Amazon S3 bucket. \n",
    "\n",
    "### Create Schemas\n",
    "\n",
    "Schemas in Amazon Personalize are defined in the Avro format. For more information, see [Apache Avro](https://avro.apache.org/docs/current/). The schema fields can be in any order but must match the order of the corresponding column headers in the data files to be imported. \n",
    "\n",
    "[docs](https://docs.aws.amazon.com/personalize/latest/dg/data-prep-formatting.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_schema = {\"type\": \"record\", \n",
    "                       \"name\": \"Interactions\",\n",
    "                       \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "                       \"fields\": [\n",
    "                       {\n",
    "                           \"name\": \"USER_ID\",\n",
    "                           \"type\": \"string\"\n",
    "                       },\n",
    "                       {\n",
    "                           \"name\": \"ITEM_ID\",\n",
    "                           \"type\": \"string\"\n",
    "                       },\n",
    "                       {\n",
    "                           \"name\": \"TIMESTAMP\",\n",
    "                           \"type\": \"long\"\n",
    "                       },\n",
    "                       {\n",
    "                           \"name\": \"EVENT_VALUE\",\n",
    "                           \"type\": \"float\"\n",
    "                       }\n",
    "                                  ],\n",
    "                                  \"version\": \"1.0\"\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:personalize:eu-west-1:873674308518:schema/interactions-schema\n"
     ]
    }
   ],
   "source": [
    "# Create schema\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"interactions-schema\",\n",
    "    schema = json.dumps(interactions_schema))\n",
    "\n",
    "# Get the ARN\n",
    "interactions_schema_arn = create_schema_response['schemaArn']\n",
    "print(interactions_schema_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_schema = {\"type\": \"record\", \n",
    "                \"name\": \"Users\",\n",
    "                \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "                \"fields\": [\n",
    "                {\n",
    "                    \"name\": \"USER_ID\",\n",
    "                    \"type\": \"string\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"BRANCHE\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"categorical\": True\n",
    "                }\n",
    "                          ],\n",
    "                          \"version\": \"1.0\"\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:personalize:eu-west-1:873674308518:schema/users-schema\n"
     ]
    }
   ],
   "source": [
    "# Create schema\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"users-schema\",\n",
    "    schema = json.dumps(users_schema))\n",
    "\n",
    "# Get the ARN\n",
    "users_schema_arn = create_schema_response['schemaArn']\n",
    "print(users_schema_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create (and wait for) Dataset Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:personalize:eu-west-1:873674308518:dataset-group/recommender-test-dataset-group\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create the Dataset Group\"\"\"\n",
    "\n",
    "create_dataset_group_response = personalize.create_dataset_group(\n",
    "    name = \"recommender-test-dataset-group\")\n",
    "\n",
    "# Get the ARN\n",
    "dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "print(dataset_group_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetGroup: CREATE PENDING\n",
      "DatasetGroup: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Wait for Dataset Group to have ACTIVE status\"\"\"\n",
    "\n",
    "max_time = time.time() + 3*60 # 3 minutes\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn)\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:personalize:eu-west-1:873674308518:dataset/recommender-test-dataset-group/INTERACTIONS\n"
     ]
    }
   ],
   "source": [
    "dataset_type = \"INTERACTIONS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    name = \"recommender-test-interactions\",\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = interactions_schema_arn)\n",
    "\n",
    "# Get the ARN\n",
    "dataset_arn_i = create_dataset_response['datasetArn']\n",
    "print(dataset_arn_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:personalize:eu-west-1:873674308518:dataset/recommender-test-dataset-group/USERS\n"
     ]
    }
   ],
   "source": [
    "dataset_type = \"USERS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    name = \"recommender-test-users\",\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = users_schema_arn)\n",
    "\n",
    "# Get the ARN\n",
    "dataset_arn_u = create_dataset_response['datasetArn']\n",
    "print(dataset_arn_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare, create, and wait for Dataset Import Job\n",
    "\n",
    "Import your training data into Amazon Personalize by first creating an empty dataset group and then an empty dataset in that dataset group. Next, create an import job that populates the dataset with data from your Amazon S3 bucket. \n",
    "\n",
    "The `roleArn` parameter specifies the AWS Identity and Access Management role that gives Amazon Personalize permissions to access your Amazon S3 bucket. (Because I have initially already set up a Personalize role in the console (see [docs](https://docs.aws.amazon.com/personalize/latest/dg/setup.html)), the first code cell is inactive and I simply load the existing roleArn in the second code cell.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Create Personalize role\"\"\"\n",
    "\n",
    "# iam = boto3.client(\"iam\")\n",
    "\n",
    "# role_name = \"PersonalizeRole\"\n",
    "# assume_role_policy_document = {\n",
    "#     \"Version\": \"2012-10-17\",\n",
    "#     \"Statement\": [\n",
    "#         {\n",
    "#           \"Effect\": \"Allow\",\n",
    "#           \"Principal\": {\n",
    "#             \"Service\": \"personalize.amazonaws.com\"\n",
    "#           },\n",
    "#           \"Action\": \"sts:AssumeRole\"\n",
    "#         }\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# create_role_response = iam.create_role(\n",
    "#     RoleName = role_name,\n",
    "#     AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    "# )\n",
    "\n",
    "# # AmazonPersonalizeFullAccess provides access to any S3 bucket with a name that includes \"personalize\" or \"Personalize\" \n",
    "# # if you would like to use a bucket with a different name, please consider creating and attaching a new policy\n",
    "# # that provides read access to your bucket or attaching the AmazonS3ReadOnlyAccess policy to the role\n",
    "# policy_arn = \"arn:aws:iam::aws:policy/service-role/AmazonPersonalizeFullAccess\"\n",
    "# iam.attach_role_policy(\n",
    "#     RoleName = role_name,\n",
    "#     PolicyArn = policy_arn\n",
    "# )\n",
    "\n",
    "# time.sleep(60) # wait for a minute to allow IAM role policy attachment to propagate\n",
    "\n",
    "# role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "# print(role_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing role ARN\n",
    "role_arn = \"arn:aws:iam::873674308518:role/PersonalizeRole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceAlreadyExistsException",
     "evalue": "An error occurred (ResourceAlreadyExistsException) when calling the CreateDatasetImportJob operation: Another resource with Arn arn:aws:personalize:eu-west-1:873674308518:dataset-import-job/interactions-dataset-import-job already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceAlreadyExistsException\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-b4f3a51108e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;34m\"dataLocation\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"s3://{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     },\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mroleArn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrole_arn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aws\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[0;32m    356\u001b[0m             \u001b[1;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aws\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m    659\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Code\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceAlreadyExistsException\u001b[0m: An error occurred (ResourceAlreadyExistsException) when calling the CreateDatasetImportJob operation: Another resource with Arn arn:aws:personalize:eu-west-1:873674308518:dataset-import-job/interactions-dataset-import-job already exists."
     ]
    }
   ],
   "source": [
    "\"\"\"Create dataset import job for interactions\"\"\"\n",
    "\n",
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"interactions-dataset-import-job\",\n",
    "    datasetArn = dataset_arn_i,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket, filename_i)\n",
    "    }, roleArn = role_arn)\n",
    "\n",
    "# Get the ARN\n",
    "dataset_import_job_arn_i = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(dataset_import_job_arn_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Wait for Dataset Import Job to Have ACTIVE Status\"\"\"\n",
    "\n",
    "max_time = time.time() + 10*60 # 10 minutes\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn_i)\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"DatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:personalize:eu-west-1:873674308518:dataset-import-job/users-dataset-import-job\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create dataset import job for users\"\"\"\n",
    "\n",
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"users-dataset-import-job\",\n",
    "    datasetArn = dataset_arn_u,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket, filename_u)\n",
    "    }, roleArn = role_arn)\n",
    "\n",
    "# Get the ARN\n",
    "dataset_import_job_arn_u = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(dataset_import_job_arn_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetImportJob: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Wait for Dataset Import Job to Have ACTIVE Status\"\"\"\n",
    "\n",
    "max_time = time.time() + 10*60 # 10 minutes\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn_u)\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"DatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a Recipe (for demo only)\n",
    "\n",
    "A _recipe_ in Amazon Personalize is made up of an algorithm with hyperparameters, and a feature transformation. Amazon Personalize provides a number of predefined recipes that allow you to make recommendations with no knowledge of machine learning.The predefined recipes are also useful for quick experimentation.\n",
    "\n",
    "(To customize the training, supply the `solutionConfig` parameter. The SolutionConfig object allows you to override the default solution and recipe parameters. This is not done here.)\n",
    "\n",
    "**NOTE:** _For this case I won't use a predefined recipe, I will let Personalize choose the optimal algorithm in the next step by calling `createSolution` with param `autoML=True`. Therefore the next codeblock is inactivated. See demo notebook for use of a predefined recipe._\n",
    "\n",
    "[docs](https://docs.aws.amazon.com/personalize/latest/dg/working-with-predefined-recipes.html) referring to available recipes:\n",
    "- popularity count (baseline model)\n",
    "- HRNN\n",
    "- HRNN-Metadata\n",
    "- HRNN-Coldstart\n",
    "- SIMS (based on item similarities, based on collaborative filtering)\n",
    "- personalized ranking (for search results, curated lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T16:29:03.047162Z",
     "start_time": "2019-07-23T16:29:03.032888Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"For demo purpose only: select an AWS HRNN\"\"\"\n",
    "\n",
    "# list_recipes_response = personalize.list_recipes()\n",
    "# recipe_arn = \"arn:aws:personalize:::recipe/aws-hrnn\"\n",
    "# list_recipes_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Wait for Solution (version)\n",
    "\n",
    "Creating a solution entails optimizing the model to deliver the best results for a specific business need. Amazon Personalize uses \"recipes\" to create these personalized solutions. (Altough in this specific case no pre-definied recipe is used.) A _solution version_ is the term Amazon Personalize uses for a trained machine learning model that makes recommendations to customers. \n",
    "\n",
    "A solution is created by calling the `CreateSolution` and `CreateSolutionVersion` operations. CreateSolution creates the configuration for training a model. CreateSolutionVersion starts the training process, which results in a specific version of the solution.\n",
    "\n",
    "[docs](https://docs.aws.amazon.com/personalize/latest/dg/training-deploying-solutions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating solution\n",
      "arn:aws:personalize:eu-west-1:873674308518:solution/recommender-test-solution\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create solution\"\"\"\n",
    "\n",
    "response = personalize.create_solution(\n",
    "    name = \"recommender-test-solution\",\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    performAutoML = True)\n",
    "\n",
    "# Get the ARN\n",
    "solution_arn = response['solutionArn']\n",
    "print(solution_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution status: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Wait for solution to have ACTIVE status\"\"\"\n",
    "\n",
    "max_time = time.time() + 20*60 # 20 minutes\n",
    "while time.time() < max_time:\n",
    "    # Use the solution ARN to get the solution status.\n",
    "    solution_description = personalize.describe_solution(solutionArn = solution_arn)['solution']\n",
    "    print('Solution status: ' + solution_description['status'])\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating solution version\n",
      "Solution version ARN: arn:aws:personalize:eu-west-1:873674308518:solution/recommender-test-solution/7aa2767b\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create solution version\"\"\"\n",
    "\n",
    "print ('Creating solution version')\n",
    "response = personalize.create_solution_version(solutionArn = solution_arn)\n",
    "solution_version_arn = response['solutionVersionArn']\n",
    "print('Solution version ARN: ' + solution_version_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save / load solution Version ARN for convenience\n",
    "\n",
    "# %store solution_version_arn\n",
    "%store -r solution_version_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution version status: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Check solution version status (manually)\"\"\"\n",
    "\n",
    "solution_version_description = personalize.describe_solution_version(\n",
    "    solutionVersionArn = solution_version_arn)['solutionVersion']\n",
    "print('Solution version status: ' + solution_version_description['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get type and metrics of solution version\n",
    "\n",
    "For each metric (not including coverage), higher numbers are better. \n",
    "- **coverage:** The proportion of unique recommended items from all queries out of the total number of unique items in the training data (includes both the Items and Interactions datasets). \n",
    "- **mean_reciprocal_rank_at_25:** The mean of the reciprocal ranks of the first relevant recommendation out of the top 25 recommendations over all queries. This metric is appropriate if you're interested in the single highest ranked recommendation. \n",
    "- **normalized_discounted_cumulative_gain_at_K:** Discounted gain assumes that recommendations lower on a list of recommendations are less relevant than higher recommendations. NDCG is between 0 - 1. This metric rewards relevant items that appear near the top of the list, because the top of a list usually draws more attention. \n",
    "- **precision_at_K:** The number of relevant recommendations out of the top K recommendations divided by K. This metric rewards precise recommendation of the relevant items.\n",
    "\n",
    "[docs](https://docs.aws.amazon.com/personalize/latest/dg/working-with-training-metrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solutionVersion': {'solutionVersionArn': 'arn:aws:personalize:eu-west-1:873674308518:solution/recommender-test-solution/7aa2767b',\n",
       "  'solutionArn': 'arn:aws:personalize:eu-west-1:873674308518:solution/recommender-test-solution',\n",
       "  'performHPO': True,\n",
       "  'performAutoML': True,\n",
       "  'recipeArn': 'arn:aws:personalize:::recipe/aws-hrnn',\n",
       "  'datasetGroupArn': 'arn:aws:personalize:eu-west-1:873674308518:dataset-group/recommender-test-dataset-group',\n",
       "  'solutionConfig': {'autoMLConfig': {'metricName': 'precision_at_25',\n",
       "    'recipeList': ['arn:aws:personalize:::recipe/aws-hrnn']}},\n",
       "  'status': 'ACTIVE',\n",
       "  'creationDateTime': datetime.datetime(2019, 7, 23, 22, 26, 34, 743000, tzinfo=tzlocal()),\n",
       "  'lastUpdatedDateTime': datetime.datetime(2019, 7, 24, 0, 2, 42, 105000, tzinfo=tzlocal())},\n",
       " 'ResponseMetadata': {'RequestId': '08f67942-71e7-4c33-a599-30c177b6af5f',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Wed, 24 Jul 2019 20:47:35 GMT',\n",
       "   'x-amzn-requestid': '08f67942-71e7-4c33-a599-30c177b6af5f',\n",
       "   'content-length': '639',\n",
       "   'connection': 'keep-alive'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get recipe type\n",
    "personalize.describe_solution_version(solutionVersionArn=solution_version_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:** Personalize has chose an HRNN, so the user metadata (industry sector) was of no use for the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"coverage\": 0.1212,\n",
      "  \"mean_reciprocal_rank_at_25\": 0.0566,\n",
      "  \"normalized_discounted_cumulative_gain_at_10\": 0.0705,\n",
      "  \"normalized_discounted_cumulative_gain_at_25\": 0.0779,\n",
      "  \"normalized_discounted_cumulative_gain_at_5\": 0.0677,\n",
      "  \"precision_at_10\": 0.0085,\n",
      "  \"precision_at_25\": 0.0046,\n",
      "  \"precision_at_5\": 0.0153\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get metrics\n",
    "get_solution_metrics_response = personalize.get_solution_metrics(\n",
    "    solutionVersionArn = solution_version_arn)\n",
    "\n",
    "print(json.dumps(get_solution_metrics_response['metrics'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and wait for campaign\n",
    "\n",
    "You create a campaign by deploying a solution version. [docs](https://docs.aws.amazon.com/personalize/latest/dg/campaigns.html\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: recommender-test-campaign\n",
      "ARN: arn:aws:personalize:eu-west-1:873674308518:campaign/recommender-test-campaign\n",
      "Status: CREATE PENDING\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create campaign\"\"\"\n",
    "\n",
    "create_campaign_response = personalize.create_campaign(\n",
    "    name = \"recommender-test-campaign\",\n",
    "    solutionVersionArn = solution_version_arn,\n",
    "    minProvisionedTPS = 1)\n",
    "\n",
    "campaign_arn = create_campaign_response['campaignArn']\n",
    "campaign_description = personalize.describe_campaign(campaignArn = campaign_arn)['campaign']\n",
    "print('Name: ' + campaign_description['name'])\n",
    "print('ARN: ' + campaign_description['campaignArn'])\n",
    "print('Status: ' + campaign_description['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'campaign_arn' (str)\n"
     ]
    }
   ],
   "source": [
    "# Save / load campaign ARN for convenience\n",
    "\n",
    "# %store campaign_arn\n",
    "%store -r campaign_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Campaign: CREATE PENDING\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Wait for campaign to have ACTIVE status\"\"\"\n",
    "\n",
    "max_time = time.time() + 20*60 # 20 minutes\n",
    "while time.time() < max_time:\n",
    "    describe_campaign_response = personalize.describe_campaign(\n",
    "        campaignArn = campaign_arn)\n",
    "    status = describe_campaign_response[\"campaign\"][\"status\"]\n",
    "    print(\"Campaign: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Recommendations\n",
    "\n",
    "To get recommendations, call the `GetRecommendations` API. Supply either the user ID or item ID, dependent on the recipe type used to create the solution the campaign is based on. The solution backing the campaign must have been created using a recipe of type USER_PERSONALIZATION or RELATED_ITEMS. That is the case here. For more information, see Using Predefined Recipes.\n",
    "\n",
    "(Note: If the solution backing the campaign has been created using a recipe of type PERSONALIZED_RANKING, you can instead of getting recommendations get a personalized ranking - a list of recommended items that are re-ranked for a specific user. To get personalized rankings, call the `GetPersonalizedRanking` API.) \n",
    "\n",
    "[docs](https://docs.aws.amazon.com/personalize/latest/dg/getting-recommendations.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>Nettowert</th>\n",
       "      <th>db_18</th>\n",
       "      <th>Kunde_unique</th>\n",
       "      <th>Datum_unique</th>\n",
       "      <th>Faktura_unique</th>\n",
       "      <th>Pos_sum</th>\n",
       "      <th>umsatz_18</th>\n",
       "      <th>u_abw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000351</td>\n",
       "      <td>Ankörn-Schablone MultiBlue für Montagepl</td>\n",
       "      <td>10.13</td>\n",
       "      <td>0.175994</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>561.95</td>\n",
       "      <td>0.018027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002809</td>\n",
       "      <td>Seitenrolle ø 30mm, max. 40kg Bauhöhe 31</td>\n",
       "      <td>289.88</td>\n",
       "      <td>0.830395</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>255</td>\n",
       "      <td>285.31</td>\n",
       "      <td>1.016018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002813</td>\n",
       "      <td>Kastenrolle ø 30mm, max. 35kg Bauhöhe 33</td>\n",
       "      <td>208.57</td>\n",
       "      <td>0.572860</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>335.37</td>\n",
       "      <td>0.621910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002814</td>\n",
       "      <td>Kastenrolle ø 50mm, max. 50kg Bauhöhe 51</td>\n",
       "      <td>160.90</td>\n",
       "      <td>0.504274</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>269</td>\n",
       "      <td>307.67</td>\n",
       "      <td>0.522963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0006344</td>\n",
       "      <td>Haftmagnetschnäpper M 74/GP 15 40 N, bra</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.156682</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.34</td>\n",
       "      <td>0.165899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                      name  Nettowert     db_18  \\\n",
       "0  0000351  Ankörn-Schablone MultiBlue für Montagepl      10.13  0.175994   \n",
       "1  0002809  Seitenrolle ø 30mm, max. 40kg Bauhöhe 31     289.88  0.830395   \n",
       "2  0002813  Kastenrolle ø 30mm, max. 35kg Bauhöhe 33     208.57  0.572860   \n",
       "3  0002814  Kastenrolle ø 50mm, max. 50kg Bauhöhe 51     160.90  0.504274   \n",
       "4  0006344  Haftmagnetschnäpper M 74/GP 15 40 N, bra       0.72  0.156682   \n",
       "\n",
       "   Kunde_unique  Datum_unique  Faktura_unique  Pos_sum  umsatz_18     u_abw  \n",
       "0             2             2               2        9     561.95  0.018027  \n",
       "1             7             7               8      255     285.31  1.016018  \n",
       "2             7             7               7       29     335.37  0.621910  \n",
       "3             1             4               4      269     307.67  0.522963  \n",
       "4             1             1               1        1       4.34  0.165899  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artikel_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Select a user and an item\"\"\"\n",
    "\n",
    "# Prepare dataframe for recommendations to display the artikel name\n",
    "artikel = artikel_raw[['id', 'name']]\n",
    "artikel.columns = ['ITEM_ID', 'TITLE']\n",
    "\n",
    "user_id, item_id, _ = sales.sample().values[0]\n",
    "# item_title = items.loc[items['ITEM_ID'] == item_id].values[0][-1]\n",
    "# print(\"USER: {}\".format(user_id))\n",
    "# print(\"ITEM: {}\".format(item_title))\n",
    "\n",
    "# items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get the recommendations\"\"\"\n",
    "\n",
    "get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "    campaignArn = campaign_arn,\n",
    "    userId = 'User ID')\n",
    "\n",
    "print(\"Recommended items\")\n",
    "for item in get_recommendations_response['itemList']:\n",
    "    print (item['itemId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "    campaignArn = campaign_arn,\n",
    "    userId = str(user_id),\n",
    "    itemId = str(item_id))\n",
    "\n",
    "item_list = get_recommendations_response['itemList']\n",
    "title_list = [items.loc[items['ITEM_ID'] == np.int(item['itemId'])].values[0][-1] for item in item_list]\n",
    "\n",
    "print(\"Recommendations: {}\".format(json.dumps(title_list, indent=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- i have to find out what type my solution version is made off --> recipe\n",
    "- I should evaluate different predefined recipes\n",
    "- generally update the comments with an overview of what we are exactly doing (boto vs. console vs. prompt)\n",
    "- for me: check what else would be possible to build a more complex model in the end"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
