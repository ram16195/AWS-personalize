{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Recommender for B2B-Retail with Amazon Personalize\n",
    "\n",
    "Building a recommender system with Amazon Personalize using the SDK for Python option (boto3). The data is the same longtail B2B-Retail set that I worked with in the \"Association Rules Mining\" ML-Project, but this time I don't reduce it to the approx 3'000 most popular items. I upload the full interactions set with roughly 74'000 different items.\n",
    "\n",
    "I also provided the industry sector of the users as meta-data but this did not improve the solution. I left the choice of algorithm and model tuning to the Personalize Service and it selected a HRNN model.\n",
    "\n",
    "For more details see the full [Documentation](https://docs.aws.amazon.com/personalize/latest/dg/what-is-personalize.html) for Amazon Personalize.\n",
    "\n",
    "Note, I learned the hard way:\n",
    "- For Europe AWS Personalize is only available in Region Ireland (eu-west-1), this is important when configuring the AWSCLI.\n",
    "- Timestamp col in interactions dataset has to be in int format\n",
    "\n",
    "\n",
    "**Data Sources:**\n",
    "\n",
    "- `data/raw/sales_total.csv`: Transaction data ('sales log') for 2017/18, this is the main data file representing the interactions between users and items.\n",
    "- `data/raw/customers_agg_2018.csv`: (Optional) data containing metadata for the users (meaning their respective business sector).\n",
    "- `data/raw/artikel_agg_2018.csv`: (Optional) data containing the names of the artikel, only needed for final output.\n",
    "\n",
    "**Changes**\n",
    "\n",
    "- 2019-07-18: Start project (in St. Ulrich, IT ;-))\n",
    "- 2019-07-25: End project (in Copenhagen, DK ;-))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries,-load-data\" data-toc-modified-id=\"Import-libraries,-load-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import libraries, load data</a></span></li><li><span><a href=\"#Prepare-and-upload-training-data-to-S3-bucket\" data-toc-modified-id=\"Prepare-and-upload-training-data-to-S3-bucket-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Prepare and upload training data to S3 bucket</a></span><ul class=\"toc-item\"><li><span><a href=\"#Upload-data-to-S3-bucket\" data-toc-modified-id=\"Upload-data-to-S3-bucket-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Upload data to S3 bucket</a></span></li></ul></li><li><span><a href=\"#Prepare-Data-Structure\" data-toc-modified-id=\"Prepare-Data-Structure-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Prepare Data Structure</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-Schemas\" data-toc-modified-id=\"Create-Schemas-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Create Schemas</a></span></li><li><span><a href=\"#Create-(and-wait-for)-Dataset-Group\" data-toc-modified-id=\"Create-(and-wait-for)-Dataset-Group-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Create (and wait for) Dataset Group</a></span></li><li><span><a href=\"#Create-Datasets\" data-toc-modified-id=\"Create-Datasets-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Create Datasets</a></span></li></ul></li><li><span><a href=\"#Prepare,-create,-and-wait-for-Dataset-Import-Job\" data-toc-modified-id=\"Prepare,-create,-and-wait-for-Dataset-Import-Job-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Prepare, create, and wait for Dataset Import Job</a></span></li><li><span><a href=\"#Select-a-Recipe-(for-demo-only)\" data-toc-modified-id=\"Select-a-Recipe-(for-demo-only)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Select a Recipe (for demo only)</a></span></li><li><span><a href=\"#Create-and-Wait-for-Solution-(version)\" data-toc-modified-id=\"Create-and-Wait-for-Solution-(version)-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Create and Wait for Solution (version)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-type-and-metrics-of-solution-version\" data-toc-modified-id=\"Get-type-and-metrics-of-solution-version-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Get type and metrics of solution version</a></span></li></ul></li><li><span><a href=\"#Create-and-wait-for-campaign\" data-toc-modified-id=\"Create-and-wait-for-campaign-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Create and wait for campaign</a></span></li><li><span><a href=\"#Get-Recommendations\" data-toc-modified-id=\"Get-Recommendations-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Get Recommendations</a></span></li><li><span><a href=\"#Appendix:-Delete-existing-resources-if-you-want-to-re-run-the-project\" data-toc-modified-id=\"Appendix:-Delete-existing-resources-if-you-want-to-re-run-the-project-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Appendix: Delete existing resources if you want to re-run the project</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries, load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T20:14:10.575455Z",
     "start_time": "2019-07-20T20:14:03.612013Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries, get personalize boto3 client\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')\n",
    "\n",
    "# Display settings\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "interactions_raw = pd.read_csv('data/raw/sales_total.csv', parse_dates=['Fakturadatum'])\n",
    "users_raw = pd.read_csv('data/raw/customers_agg_2018.csv')\n",
    "artikel_raw = pd.read_csv('data/raw/artikel_agg_2018.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T14:25:07.736597Z",
     "start_time": "2019-07-18T14:25:05.697778Z"
    }
   },
   "source": [
    "## Prepare and upload training data to S3 bucket\n",
    "\n",
    "Amazon Personalize recognizes three types of historical datasets. Each type has an associated schema (see next section) with a name key whose value matches the dataset type. The three types are: \n",
    "- **Users:** This dataset is intended to provide metadata about your users. This includes information such as age, gender, and loyalty membership, among others, which can be important signals in personalization systems. \n",
    "- **Items:** This dataset is intended to provide metadata about your items. This includes information such as price, SKU type, and availability, among others. \n",
    "- **Interactions:** This dataset is intended to provide historical interaction data between users and items. \n",
    "\n",
    "The Users and Items dataset types are known as metadata types and are only used by certain recipes. As we have no relevant metadata for items, we prepare 2 datasets for\n",
    "\n",
    "- interactions\n",
    "- users\n",
    "\n",
    "[docs](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prepare interaction data\"\"\"\n",
    "\n",
    "# Subset data for 2018 data only\n",
    "interactions_18_full = interactions_raw.loc[interactions_raw['Fakturadatum'].dt.year == 2018]\n",
    "interactions_18_part = interactions_18_full[['Kunde', 'Artikel', 'Fakturadatum', 'Nettowert']]\n",
    "\n",
    "# Kick out all artikel that contain str values in their code\n",
    "interactions_18_part['num'] = pd.to_numeric(interactions_18_part['Artikel'], errors='coerce')\n",
    "interactions_18 = interactions_18_part.dropna(how='any')\n",
    "interactions_18.drop(['num'], axis=1, inplace=True)\n",
    "\n",
    "# Kick-out special customers\n",
    "interactions = interactions_18.loc[interactions_18['Kunde'] > 700000]\n",
    "\n",
    "# Set datatypes\n",
    "interactions['Kunde'] = interactions['Kunde'].astype(str)\n",
    "interactions['Artikel'] = interactions['Artikel'].astype(str)\n",
    "interactions['Fakturadatum'] = interactions['Fakturadatum'].apply(lambda x: x.timestamp()).astype(int)\n",
    "interactions['Nettowert'] = interactions['Nettowert'].astype(float)\n",
    "\n",
    "# Rename Columns\n",
    "interactions = interactions.rename(columns={'Kunde': 'USER_ID', \n",
    "                                            'Artikel': 'ITEM_ID',\n",
    "                                            'Fakturadatum': 'TIMESTAMP',\n",
    "                                            'Nettowert': 'EVENT_VALUE',\n",
    "                                           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1402641, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>EVENT_VALUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1388625</th>\n",
       "      <td>8488019</td>\n",
       "      <td>5171607</td>\n",
       "      <td>1514937600</td>\n",
       "      <td>77.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388626</th>\n",
       "      <td>8488019</td>\n",
       "      <td>5171101</td>\n",
       "      <td>1514937600</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         USER_ID  ITEM_ID   TIMESTAMP  EVENT_VALUE\n",
       "1388625  8488019  5171607  1514937600         77.3\n",
       "1388626  8488019  5171101  1514937600         32.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check results\n",
    "assert interactions.isnull().sum().sum() == 0\n",
    "print(interactions.shape)\n",
    "display(interactions.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "interactions.to_csv(\"data/interim/interactions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18625, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>BRANCHE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8107232</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8155006</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   USER_ID BRANCHE\n",
       "0  8107232    15.0\n",
       "1  8155006    10.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Prepare User data\"\"\"\n",
    "\n",
    "users = users_raw[['Unnamed: 0', 'Branche']]\n",
    "users = users.rename(columns={'Unnamed: 0': 'USER_ID', \n",
    "                              'Branche': 'BRANCHE',\n",
    "                             })\n",
    "\n",
    "# Set datatypes\n",
    "users['USER_ID'] = users['USER_ID'].astype(str)\n",
    "users['BRANCHE'] = users['BRANCHE'].astype(str)\n",
    "\n",
    "# Check results\n",
    "print(users.shape)\n",
    "display(users.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "users.to_csv(\"data/interim/users.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3 bucket\n",
    "\n",
    "After you create a CSV file with your data, upload the file to your Amazon S3 bucket. This is the location that Amazon Personalize imports your data from. Amazon Personalize needs permission to access the Amazon S3 bucket, so a policy has to be attached.\n",
    "\n",
    "[docs](https://docs.aws.amazon.com/personalize/latest/dg/data-prep-upload-s3.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbuerki-01-personalize\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the list of existing buckets (optional)\n",
    "s3 = boto3.client('s3')\n",
    "response= s3.list_buckets()\n",
    "for bucket in response['Buckets']:\n",
    "    print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '8B7ACD024A1AEB4F',\n",
       "  'HostId': '1OKTq1uUJE5t0Qjjw2z4O0ac5t4B2GzgMIprw46QhqvoLlewWPXkEs0xQzWn3i/Awl4S/Tc2Exw=',\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '1OKTq1uUJE5t0Qjjw2z4O0ac5t4B2GzgMIprw46QhqvoLlewWPXkEs0xQzWn3i/Awl4S/Tc2Exw=',\n",
       "   'x-amz-request-id': '8B7ACD024A1AEB4F',\n",
       "   'date': 'Fri, 26 Jul 2019 21:05:31 GMT',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 1}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Specify a s3 Bucket and attach policy to it\"\"\"\n",
    "\n",
    "bucket = \"rbuerki-01-personalize\"  # name of my S3 bucket\n",
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket, Policy=json.dumps(policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T20:12:10.749057Z",
     "start_time": "2019-07-20T20:12:10.733435Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Upload interactions data\"\"\"\n",
    "\n",
    "filename_i = 'interactions.csv' \n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "    filename_i).upload_file(\"data/interim/{}\".format(filename_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Upload user data\"\"\"\n",
    "\n",
    "filename_u = 'users.csv'\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "    filename_u).upload_file(\"data/interim/{}\".format(filename_u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data Structure\n",
    "\n",
    "\n",
    "Import your training data into Amazon Personalize by first creating matching data schemas for your sets, then an empty dataset group and then an empty dataset in that dataset group. Next, create an import job that populates the dataset with data from your Amazon S3 bucket. \n",
    "\n",
    "### Create Schemas\n",
    "\n",
    "Schemas in Amazon Personalize are defined in the Avro format. For more information, see [Apache Avro](https://avro.apache.org/docs/current/). The schema fields can be in any order but must match the order of the corresponding column headers in the data files to be imported. \n",
    "\n",
    "[docs](https://docs.aws.amazon.com/personalize/latest/dg/data-prep-formatting.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_schema = {\"type\": \"record\", \n",
    "                       \"name\": \"Interactions\",\n",
    "                       \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "                       \"fields\": [\n",
    "                       {\n",
    "                           \"name\": \"USER_ID\",\n",
    "                           \"type\": \"string\"\n",
    "                       },\n",
    "                       {\n",
    "                           \"name\": \"ITEM_ID\",\n",
    "                           \"type\": \"string\"\n",
    "                       },\n",
    "                       {\n",
    "                           \"name\": \"TIMESTAMP\",\n",
    "                           \"type\": \"long\"\n",
    "                       },\n",
    "                       {\n",
    "                           \"name\": \"EVENT_VALUE\",\n",
    "                           \"type\": \"float\"\n",
    "                       }\n",
    "                                  ],\n",
    "                                  \"version\": \"1.0\"\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T14:25:10.599629Z",
     "start_time": "2019-07-31T14:25:10.584011Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create schema\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"interactions-schema\",\n",
    "    schema = json.dumps(interactions_schema))\n",
    "\n",
    "# Get the ARN\n",
    "interactions_schema_arn = create_schema_response['schemaArn']\n",
    "print(interactions_schema_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_schema = {\"type\": \"record\", \n",
    "                \"name\": \"Users\",\n",
    "                \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "                \"fields\": [\n",
    "                {\n",
    "                    \"name\": \"USER_ID\",\n",
    "                    \"type\": \"string\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"BRANCHE\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"categorical\": True\n",
    "                }\n",
    "                          ],\n",
    "                          \"version\": \"1.0\"\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T14:25:28.717622Z",
     "start_time": "2019-07-31T14:25:28.702001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create schema\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"users-schema\",\n",
    "    schema = json.dumps(users_schema))\n",
    "\n",
    "# Get the ARN\n",
    "users_schema_arn = create_schema_response['schemaArn']\n",
    "print(users_schema_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create (and wait for) Dataset Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T14:25:40.277826Z",
     "start_time": "2019-07-31T14:25:40.246588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Create the Dataset Group'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Create the Dataset Group\"\"\"\n",
    "\n",
    "create_dataset_group_response = personalize.create_dataset_group(\n",
    "    name = \"recommender-test-dataset-group\")\n",
    "\n",
    "# Get the ARN\n",
    "dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "print(dataset_group_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetGroup: CREATE PENDING\n",
      "DatasetGroup: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Wait for Dataset Group to have ACTIVE status\"\"\"\n",
    "\n",
    "max_time = time.time() + 3*60 # 3 minutes\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn)\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T14:26:15.433507Z",
     "start_time": "2019-07-31T14:26:15.417886Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_type = \"INTERACTIONS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    name = \"recommender-test-interactions\",\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = interactions_schema_arn)\n",
    "\n",
    "# Get the ARN\n",
    "dataset_arn_i = create_dataset_response['datasetArn']\n",
    "print(dataset_arn_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T14:26:43.752076Z",
     "start_time": "2019-07-31T14:26:43.736425Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_type = \"USERS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    name = \"recommender-test-users\",\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = users_schema_arn)\n",
    "\n",
    "# Get the ARN\n",
    "dataset_arn_u = create_dataset_response['datasetArn']\n",
    "print(dataset_arn_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare, create, and wait for Dataset Import Job\n",
    "\n",
    "Import your training data into Amazon Personalize by first creating an empty dataset group and then an empty dataset in that dataset group. Next, create an import job that populates the dataset with data from your Amazon S3 bucket. \n",
    "\n",
    "The `roleArn` parameter specifies the AWS Identity and Access Management role that gives Amazon Personalize permissions to access your Amazon S3 bucket. (Because I have initially already set up a Personalize role in the console (see [docs](https://docs.aws.amazon.com/personalize/latest/dg/setup.html)), the first code cell is inactive and I simply load the existing roleArn in the second code cell.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Create Personalize role\"\"\"\n",
    "\n",
    "# iam = boto3.client(\"iam\")\n",
    "\n",
    "# role_name = \"PersonalizeRole\"\n",
    "# assume_role_policy_document = {\n",
    "#     \"Version\": \"2012-10-17\",\n",
    "#     \"Statement\": [\n",
    "#         {\n",
    "#           \"Effect\": \"Allow\",\n",
    "#           \"Principal\": {\n",
    "#             \"Service\": \"personalize.amazonaws.com\"\n",
    "#           },\n",
    "#           \"Action\": \"sts:AssumeRole\"\n",
    "#         }\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# create_role_response = iam.create_role(\n",
    "#     RoleName = role_name,\n",
    "#     AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    "# )\n",
    "\n",
    "# # AmazonPersonalizeFullAccess provides access to any S3 bucket with a name that includes \"personalize\" or \"Personalize\" \n",
    "# # if you would like to use a bucket with a different name, please consider creating and attaching a new policy\n",
    "# # that provides read access to your bucket or attaching the AmazonS3ReadOnlyAccess policy to the role\n",
    "# policy_arn = \"arn:aws:iam::aws:policy/service-role/AmazonPersonalizeFullAccess\"\n",
    "# iam.attach_role_policy(\n",
    "#     RoleName = role_name,\n",
    "#     PolicyArn = policy_arn\n",
    "# )\n",
    "\n",
    "# time.sleep(60) # wait for a minute to allow IAM role policy attachment to propagate\n",
    "\n",
    "# role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "# print(role_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing role ARN\n",
    "role_arn = \"\" # hidden from public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create dataset import job for interactions\"\"\"\n",
    "\n",
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"interactions-dataset-import-job\",\n",
    "    datasetArn = dataset_arn_i,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket, filename_i)\n",
    "    }, roleArn = role_arn)\n",
    "\n",
    "# Get the ARN\n",
    "dataset_import_job_arn_i = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(dataset_import_job_arn_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Wait for Dataset Import Job to Have ACTIVE Status\"\"\"\n",
    "\n",
    "max_time = time.time() + 10*60 # 10 minutes\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn_i)\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"DatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T14:27:25.741405Z",
     "start_time": "2019-07-31T14:27:25.725791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Create dataset import job for users'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Create dataset import job for users\"\"\"\n",
    "\n",
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"users-dataset-import-job\",\n",
    "    datasetArn = dataset_arn_u,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket, filename_u)\n",
    "    }, roleArn = role_arn)\n",
    "\n",
    "# Get the ARN\n",
    "dataset_import_job_arn_u = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(dataset_import_job_arn_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetImportJob: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Wait for Dataset Import Job to Have ACTIVE Status\"\"\"\n",
    "\n",
    "max_time = time.time() + 10*60 # 10 minutes\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn_u)\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"DatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a Recipe (for demo only)\n",
    "\n",
    "A _recipe_ in Amazon Personalize is made up of an algorithm with hyperparameters, and a feature transformation. Amazon Personalize provides a number of predefined recipes that allow you to make recommendations with no knowledge of machine learning.The predefined recipes are also useful for quick experimentation.\n",
    "\n",
    "(To customize the training, supply the `solutionConfig` parameter. The SolutionConfig object allows you to override the default solution and recipe parameters. This is not done here.)\n",
    "\n",
    "**NOTE:** _For this case I won't use a predefined recipe, I will let Personalize choose the optimal algorithm in the next step by calling `createSolution` with param `autoML=True`. Therefore the next codeblock is inactivated. See demo notebook for use of a predefined recipe._\n",
    "\n",
    "[docs](https://docs.aws.amazon.com/personalize/latest/dg/working-with-predefined-recipes.html) referring to available recipes:\n",
    "- popularity count (baseline model)\n",
    "- HRNN\n",
    "- HRNN-Metadata\n",
    "- HRNN-Coldstart\n",
    "- SIMS (based on item similarities, based on collaborative filtering)\n",
    "- personalized ranking (for search results, curated lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T16:29:03.047162Z",
     "start_time": "2019-07-23T16:29:03.032888Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"For demo purpose only: select an AWS HRNN\"\"\"\n",
    "\n",
    "# list_recipes_response = personalize.list_recipes()\n",
    "# recipe_arn = \"arn:aws:personalize:::recipe/aws-hrnn\"\n",
    "# list_recipes_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Wait for Solution (version)\n",
    "\n",
    "Creating a solution entails optimizing the model to deliver the best results for a specific business need. Amazon Personalize uses \"recipes\" to create these personalized solutions. (Altough in this specific case no pre-definied recipe is used.) A _solution version_ is the term Amazon Personalize uses for a trained machine learning model that makes recommendations to customers. \n",
    "\n",
    "A solution is created by calling the `CreateSolution` and `CreateSolutionVersion` operations. CreateSolution creates the configuration for training a model. CreateSolutionVersion starts the training process, which results in a specific version of the solution.\n",
    "\n",
    "[docs](https://docs.aws.amazon.com/personalize/latest/dg/training-deploying-solutions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T14:27:43.235960Z",
     "start_time": "2019-07-31T14:27:43.204730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Create solution'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Create solution\"\"\"\n",
    "\n",
    "response = personalize.create_solution(\n",
    "    name = \"recommender-test-solution\",\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    performAutoML = True)\n",
    "\n",
    "# Get the ARN\n",
    "solution_arn = response['solutionArn']\n",
    "print(solution_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution status: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Wait for solution to have ACTIVE status\"\"\"\n",
    "\n",
    "max_time = time.time() + 20*60 # 20 minutes\n",
    "while time.time() < max_time:\n",
    "    # Use the solution ARN to get the solution status.\n",
    "    solution_description = personalize.describe_solution(solutionArn = solution_arn)['solution']\n",
    "    print('Solution status: ' + solution_description['status'])\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T14:28:04.185565Z",
     "start_time": "2019-07-31T14:28:04.154321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating solution version\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create solution version\"\"\"\n",
    "\n",
    "print ('Creating solution version')\n",
    "response = personalize.create_solution_version(solutionArn = solution_arn)\n",
    "solution_version_arn = response['solutionVersionArn']\n",
    "print('Solution version ARN: ' + solution_version_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save / load solution Version ARN for convenience\n",
    "\n",
    "# %store solution_version_arn\n",
    "%store -r solution_version_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution version status: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Check solution version status (manually)\"\"\"\n",
    "\n",
    "solution_version_description = personalize.describe_solution_version(\n",
    "    solutionVersionArn = solution_version_arn)['solutionVersion']\n",
    "print('Solution version status: ' + solution_version_description['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get type and metrics of solution version\n",
    "\n",
    "For each metric (not including coverage), higher numbers are better. \n",
    "- **coverage:** The proportion of unique recommended items from all queries out of the total number of unique items in the training data (includes both the Items and Interactions datasets). \n",
    "- **mean_reciprocal_rank_at_25:** The mean of the reciprocal ranks of the first relevant recommendation out of the top 25 recommendations over all queries. This metric is appropriate if you're interested in the single highest ranked recommendation. \n",
    "- **normalized_discounted_cumulative_gain_at_K:** Discounted gain assumes that recommendations lower on a list of recommendations are less relevant than higher recommendations. NDCG is between 0 - 1. This metric rewards relevant items that appear near the top of the list, because the top of a list usually draws more attention. \n",
    "- **precision_at_K:** The number of relevant recommendations out of the top K recommendations divided by K. This metric rewards precise recommendation of the relevant items.\n",
    "\n",
    "[docs](https://docs.aws.amazon.com/personalize/latest/dg/working-with-training-metrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T14:28:47.977934Z",
     "start_time": "2019-07-31T14:28:47.962345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get recipe type\n",
    "personalize.describe_solution_version(solutionVersionArn=solution_version_arn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:** Personalize has chose an HRNN, so the user metadata (industry sector) was of no use for the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"coverage\": 0.1212,\n",
      "  \"mean_reciprocal_rank_at_25\": 0.0566,\n",
      "  \"normalized_discounted_cumulative_gain_at_10\": 0.0705,\n",
      "  \"normalized_discounted_cumulative_gain_at_25\": 0.0779,\n",
      "  \"normalized_discounted_cumulative_gain_at_5\": 0.0677,\n",
      "  \"precision_at_10\": 0.0085,\n",
      "  \"precision_at_25\": 0.0046,\n",
      "  \"precision_at_5\": 0.0153\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get metrics\n",
    "get_solution_metrics_response = personalize.get_solution_metrics(\n",
    "    solutionVersionArn = solution_version_arn)\n",
    "\n",
    "print(json.dumps(get_solution_metrics_response['metrics'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and wait for campaign\n",
    "\n",
    "You create a campaign by deploying a solution version. [docs](https://docs.aws.amazon.com/personalize/latest/dg/campaigns.html\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T14:29:22.439462Z",
     "start_time": "2019-07-31T14:29:22.408224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Create campaign'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Create campaign\"\"\"\n",
    "\n",
    "create_campaign_response = personalize.create_campaign(\n",
    "    name = \"recommender-test-campaign\",\n",
    "    solutionVersionArn = solution_version_arn,\n",
    "    minProvisionedTPS = 1)\n",
    "\n",
    "campaign_arn = create_campaign_response['campaignArn']\n",
    "campaign_description = personalize.describe_campaign(campaignArn = campaign_arn)['campaign']\n",
    "print('Name: ' + campaign_description['name'])\n",
    "print('ARN: ' + campaign_description['campaignArn'])\n",
    "print('Status: ' + campaign_description['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save / load campaign ARN for convenience\n",
    "\n",
    "# %store campaign_arn\n",
    "%store -r campaign_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Campaign: CREATE PENDING\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Wait for campaign to have ACTIVE status\"\"\"\n",
    "\n",
    "max_time = time.time() + 20*60 # 20 minutes\n",
    "while time.time() < max_time:\n",
    "    describe_campaign_response = personalize.describe_campaign(\n",
    "        campaignArn = campaign_arn)\n",
    "    status = describe_campaign_response[\"campaign\"][\"status\"]\n",
    "    print(\"Campaign: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Recommendations\n",
    "\n",
    "To get recommendations, call the `GetRecommendations` API. Supply either the user ID or item ID, dependent on the recipe type used to create the solution the campaign is based on. The solution backing the campaign must have been created using a recipe of type USER_PERSONALIZATION or RELATED_ITEMS. That is the case here. For more information, see Using Predefined Recipes.\n",
    "\n",
    "(Note: If the solution backing the campaign has been created using a recipe of type PERSONALIZED_RANKING, you can instead of getting recommendations get a personalized ranking - a list of recommended items that are re-ranked for a specific user. To get personalized rankings, call the `GetPersonalizedRanking` API.) \n",
    "\n",
    "[docs](https://docs.aws.amazon.com/personalize/latest/dg/getting-recommendations.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataframe for recommendations to display the artikel name\n",
    "artikel = artikel_raw[['id', 'name']]\n",
    "artikel.columns = ['ITEM_ID', 'TITLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: 8826031\n",
      "ITEM: Deckenklips, Kunststoff\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Select a random user-item combination from transaction set\"\"\"\n",
    "\n",
    "user_id, item_id = interactions.iloc[:,:2].sample().values[0]\n",
    "item_title = artikel.loc[artikel['ITEM_ID'] == item_id].values[0][-1]\n",
    "\n",
    "print(\"USER:\", user_id)\n",
    "print(\"ITEM:\",item_title)\n",
    "\n",
    "# items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artikel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decoupier-Sägeblätter 1.5 X 130 X 0.48mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hebel-Revolverlochzange 250 mm mit 6 Loc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spannschrauben M5 x14 zu Laubsägebogen S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nachschlüssel SR100 Kaba star für Schlie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fugen- und Furnierleim COLLANO FL 330, G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Silberlot cadmiumfrei 1,5 mm Pk. zu 100g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Möbelgleiter Basis Modul ø20mm Kunst.sch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wendehobelmesser HM Länge 82 mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bandsägeblatt JET JWBS-14 10 x 2560 x 0.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bandsägeblatt 10 x 1875mm gebrauchsferti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Korundpapier, Korn 80 1960 Siarex, 230 X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Palettenstretchfolien, 100 mm 150 m, Ker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Schleifscheiben SIAFAST K 80 1960 Siarex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Normalgestänge, silberfarbig zu DORMA TS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Laubsägebogen STUTZ 300 mm Ausladung, Ho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Einspannvorrichtung OK-TOOLS für Laubsäg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Rastfeststelleinheit zu Gleitschienen DO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Akku Schlagbohrschrauber DEWALT DCD795D2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Glaspolitur Radora Brillant 500 g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Normalgestänge silberfarbig zu GEZE TS 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Japansäge SHOGUN, Blatt 265 mm mit Revol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sägeblatt 550 mm, Zahnweite 1.75 mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Zuschlag Zylindernachbest. Kaba star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Gummipuffer weiss ø 20/10 mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Gliedermeter Hultafors, Birke, Länge 2 m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Artikel\n",
       "1   Decoupier-Sägeblätter 1.5 X 130 X 0.48mm\n",
       "2   Hebel-Revolverlochzange 250 mm mit 6 Loc\n",
       "3   Spannschrauben M5 x14 zu Laubsägebogen S\n",
       "4   Nachschlüssel SR100 Kaba star für Schlie\n",
       "5   Fugen- und Furnierleim COLLANO FL 330, G\n",
       "6   Silberlot cadmiumfrei 1,5 mm Pk. zu 100g\n",
       "7   Möbelgleiter Basis Modul ø20mm Kunst.sch\n",
       "8            Wendehobelmesser HM Länge 82 mm\n",
       "9   Bandsägeblatt JET JWBS-14 10 x 2560 x 0.\n",
       "10  Bandsägeblatt 10 x 1875mm gebrauchsferti\n",
       "11  Korundpapier, Korn 80 1960 Siarex, 230 X\n",
       "12  Palettenstretchfolien, 100 mm 150 m, Ker\n",
       "13  Schleifscheiben SIAFAST K 80 1960 Siarex\n",
       "14  Normalgestänge, silberfarbig zu DORMA TS\n",
       "15  Laubsägebogen STUTZ 300 mm Ausladung, Ho\n",
       "16  Einspannvorrichtung OK-TOOLS für Laubsäg\n",
       "17  Rastfeststelleinheit zu Gleitschienen DO\n",
       "18  Akku Schlagbohrschrauber DEWALT DCD795D2\n",
       "19         Glaspolitur Radora Brillant 500 g\n",
       "20  Normalgestänge silberfarbig zu GEZE TS 2\n",
       "21  Japansäge SHOGUN, Blatt 265 mm mit Revol\n",
       "22       Sägeblatt 550 mm, Zahnweite 1.75 mm\n",
       "23      Zuschlag Zylindernachbest. Kaba star\n",
       "24              Gummipuffer weiss ø 20/10 mm\n",
       "25  Gliedermeter Hultafors, Birke, Länge 2 m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Get the recommendations\"\"\"\n",
    "\n",
    "get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "    campaignArn = campaign_arn,\n",
    "    userId = 'User ID')\n",
    "\n",
    "item_list = get_recommendations_response['itemList']\n",
    "recommendations = pd.DataFrame(\n",
    "    [artikel.loc[artikel['ITEM_ID'] == item['itemId']].values[0][-1] for item in item_list], \n",
    "    columns=['Artikel'], \n",
    "    index = np.arange(1,26,1))\n",
    "\n",
    "print(\"Recommended items\")\n",
    "display(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Delete existing resources if you want to re-run the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First things first: Delete existing resources before (re-)running the project\n",
    "group = 'recommender-test-dataset-group'\n",
    "set_list = ['INTERACTIONS', 'USERS']\n",
    "schema_list = ['interactions-schema', 'users-schema']\n",
    "\n",
    "try:\n",
    "    for set in set_list:\n",
    "        personalize.delete_dataset(\n",
    "            datasetArn=\"arn:aws:personalize:eu-west-1:873674308518:dataset/{}/{}\".format(group, set))\n",
    "        print('set deleted')\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    personalize.delete_dataset_group(\n",
    "        datasetGroupArn=\"arn:aws:personalize:eu-west-1:873674308518:dataset-group/{}\".format(group))\n",
    "    print('group deleted')\n",
    "except Exception:\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema deleted\n",
      "schema deleted\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for schema in ['interactions-schema', 'users-schema']:\n",
    "        personalize.delete_schema(\n",
    "            schemaArn=\"arn:aws:personalize:eu-west-1:873674308518:schema/{}\".format(schema))\n",
    "        print('schema deleted')\n",
    "except Exception:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
